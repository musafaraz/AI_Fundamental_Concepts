


import numpy as np # Importing NumPy for numerical operations
import pandas as pd # Importing Pandas for data manipulation
import matplotlib.pyplot as plt # Importing Matplotlib for plotting
from collections import Counter # Importing Counter from the collections module for counting occurrences
from matplotlib.colors import Normalize # Importing Normalize from Matplotlib to normalize data for color mapping
from sklearn.cluster import KMeans # Importing the KMeans clustering algorithm from scikit-learn
import seaborn as sns # Importing Seaborn for statistical data visualization
from sklearn.neighbors import NearestNeighbors # Importing NearestNeighbors from scikit-learn for unsupervised nearest neighbors learning
from random import sample # Importing necessary modules for random sampling and mathematical operations
from numpy.random import uniform
from math import isnan
from sklearn.preprocessing import StandardScaler # Importing StandardScaler from scikit-learn for feature scaling
import warnings # For Suppressing warnings
warnings.filterwarnings('ignore')

############################
# Initial Data Exploration # 
############################

# Load the dataset into a DataFrame
df = pd.read_csv('C:/Users/Faraz Yusuf Khan/Desktop/Data/country_data.csv')
# Check for missing values in the dataset
print('Total missing values in the dataset:', sum(df.isna().sum()))
# Check for the data type of each column
data_types = df.dtypes
print('Data types of each column:')
print(data_types)
# Drop the first column (as it is an unsuitable varaible)
a = df.iloc[:, 1:]
# Select specific columns (Income and GDP) to create the feature matrix X
X = df.iloc[:, [5, 9]].values

#####################
# K-Means Algorithm #
#####################

# Specify the number of clusters for the KMeans algorithm
num_clusters = 2

# Initialize the KMeans algorithm with the specified number of clusters
# n_init='auto' automatically selects the number of initializations for the KMeans algorithm
# random_state is set for reproducibility
kmeans = KMeans(n_clusters=num_clusters, n_init='auto', random_state=5)

# Run the KMeans algorithm on the data X
kmeans.fit(X)

# Print the center positions of the identified clusters
centers = kmeans.cluster_centers_
print('Centroids:\n', centers, '\n')

# Predict the cluster to which a test point belongs
test_point = np.array([[3, 1]])
prediction = kmeans.predict(test_point)
print('Prediction:', prediction, '\n')

# Visualize the clustering results using Matplotlib
fig, ax = plt.subplots()

# Store the normalization of the color encodings
# based on the number of clusters
nm = Normalize(vmin=0, vmax=len(centers) - 1)

# Specify markers for each cluster
markers = ['o', 's']

# Plot the clustered data with different markers
for i in range(len(centers)):
    # Extract data points belonging to the current cluster
    cluster_points = X[kmeans.labels_ == i]
    
    # Scatter plot for each cluster with unique markers
    ax.scatter(cluster_points[:, 0], cluster_points[:, 1], s=50, cmap='plasma', norm=nm, marker=markers[i], label=f'Cluster {i + 1}')

# Plot the test point with a distinctive marker
ax.scatter(test_point[:, 0], test_point[:, 1], marker='*', s=50, cmap='plasma', norm=nm, label='Test Point')

# Plot the centroids and label them with cluster numbers
for i in range(centers.shape[0]):
    ax.text(centers[i, 0], centers[i, 1], str(i), c='black',
            bbox=dict(boxstyle="round", facecolor='white', edgecolor='black'))

# Set labels for the axes
ax.set_xlabel('Income')
ax.set_ylabel('GDP')

# Display legend and save the figure
ax.legend()
fig.savefig('cluster_plot_colorblind_friendly.png')

# Show the plot
plt.show()


##################################
### Complex K- Means Algorithm ###
##################################

# Convert 'exports', 'health', 'imports' from percentages to actual values
df[['exports', 'health', 'imports']] = df[['exports', 'health', 'imports']].apply(lambda x: x * df["gdpp"] / 100)

# Confirm changes to the dataframe by printing the first few rows
print(df.head())

# Correlation analysis and heatmap
data = df.iloc[:, 1:]  # Exclude the 'country' column for correlation analysis
correlation_matrix = data.corr()

# Plot a heatmap to visualize correlations
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap (Excluding First Column)')
plt.show()

# Testing for skewness and plotting distribution plots
fig, ax = plt.subplots(figsize=(12, 10))

for i, feature in enumerate(df.drop('country', axis=1).columns):
    plt.subplot(6, 3, i+1)
    plt.subplots_adjust(hspace=2.0)
    sns.distplot(df[feature])
    plt.tight_layout()

# Outlier handling: Visualizing and identifying outliers through BoxPlots
columns_to_plot = data.columns[0:]
plt.figure(figsize=(12, 6))

# Plot boxplots for each feature to visualize outliers
for column in columns_to_plot:
    plt.subplot(1, len(columns_to_plot), columns_to_plot.get_loc(column) + 1)
    data.boxplot(column=column)
    plt.title(column)

plt.tight_layout()
plt.show()


# Since removing numerous outliers may lead to insufficient data for analysis,
# perform a floor operation for values lower than (Q1 - 1.5 * IQR) and
# a ceiling operation for values higher than (Q3 + 1.5 * IQR).
df_updated = df.iloc[:, :]

def outliers_for_features(df, col):    
    # Calculate the first and third quartiles (Q1 and Q3)
    Q1 = df.loc[:, col].quantile(0.25)
    Q3 = df.loc[:, col].quantile(0.75)
    
    # Calculate the upper and lower limits for identifying outliers
    upper_limit = Q3 + 1.5 * (Q3 - Q1)
    lower_limit = Q1 - 1.5 * (Q3 - Q1)
    
    # Apply the floor and ceiling operations to handle outliers
    return df_updated[col].apply(lambda x: upper_limit if x > upper_limit else lower_limit if x < lower_limit else x)

# Apply outlier handling for specified columns
for col in ['life_expec', 'inflation', 'total_fer', 'exports', 'imports', 'health', 'income', 'gdpp']:
    df_updated[col] = outliers_for_features(df, col)

# Define a function for calculating the Hopkins statistic
def hopkins(X):
    d = X.shape[1]  # Get the number of columns (dimensions)
    n = len(X)  # Get the number of rows
    m = int(0.1 * n)  # Calculate the number of random points to sample
    
    # Fit a Nearest Neighbors model on the dataset
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)

    # Randomly sample 'm' points from the dataset
    rand_X = sample(range(0, n, 1), m)
    
    ujd = []
    wjd = []
    for j in range(0, m):
        # Calculate the distance to the nearest neighbor for a uniform random point
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X, axis=0), np.amax(X, axis=0), d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        
        # Calculate the distance to the nearest neighbor for a random point from the dataset
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])
 
    H = sum(ujd) / (sum(ujd) + sum(wjd))  # Calculate the Hopkins statistic
    
    if isnan(H):
        print(ujd, wjd)
        H = 0  # Handle the case where the Hopkins statistic is NaN
    
    return H

# Calculate the Hopkins statistic for the updated DataFrame 'df_updated' with 'country' column dropped
hopkins_statistic = hopkins(df_updated.drop('country', axis=1))
print("Hopkins statistic for the dataset is:", hopkins_statistic) #High value of Hopkins Statistics implements that dataset has high tendency to cluster


# Perform standard scaling on the selected columns of the 'df_updated' DataFrame
standard_scaler = StandardScaler()
df_scaled = standard_scaler.fit_transform(df_updated.iloc[:, 1:])

# Choosing k-value for K means algorithm
## Define a function to plot the Inertia vs. Number of Clusters
def plot_inertia_vs_clusters(data):
    ssd = []  # List to store the Sum of Squared Distances (Inertia)
    num_of_clusters = list(range(2, 10))  # List of cluster numbers to test

    # Iterate through different cluster numbers
    for n in num_of_clusters:
        # Fit a K-Means model with 'n' clusters to the data
        km = KMeans(n_clusters=n, max_iter=50, random_state=101).fit(data)
        # Append the Inertia (within-cluster sum of squares) to the list
        ssd.append(km.inertia_)

    # Create a new figure for the plot
    plt.figure()

    # Plot the Inertia values for different cluster numbers
    plt.plot(num_of_clusters, ssd, marker='o')

    # Annotate the points on the plot with their Inertia values
    for xy in zip(num_of_clusters, ssd):
        plt.annotate(text=round(xy[1], 3), xy=xy, textcoords='data')

    # Set the labels and title for the plot
    plt.xlabel("Number of clusters")
    plt.ylabel("Inertia (WCSS)")  # Inertia is within-cluster sum of squares
    plt.title("Number of Clusters vs. Inertia (WCSS)")

    # Show the last plot separately
    plt.show()

# Call the function with your data
plot_inertia_vs_clusters(df_scaled)
  

# Building K Means model with 3 clusters
km = KMeans(n_clusters=3, max_iter=500 , random_state = 101)
km.fit(df_scaled)

# Get cluster centers
centers = km.cluster_centers_

# Print the center positions of clusters
print("Cluster Centers:")
for i, center in enumerate(centers):
    print(f"Cluster {i+1}: {center}")


# Print number of clusters
print("Number of clusters:", km.n_clusters)

# Assuming df_scaled is your scaled dataset
km = KMeans(n_clusters=3, max_iter=100, random_state=101)
km.fit(df_scaled)

# Get cluster labels
labels = km.labels_

# Create a 3D scatter plot
fig = plt.figure(figsize=(8, 6))
ax = fig.add_subplot(111, projection='3d')

# Choose a color-blind friendly colormap
cmap = 'cividis'

# Define different markers for each cluster
markers = ['o', 's', '^', 'D', 'v', 'p', '*', '+']

# Scatter plot for each cluster with different markers
for i in range(len(centers)):
    cluster_points = df_scaled[labels == i]
    ax.scatter(cluster_points[:, 0], cluster_points[:, 1], cluster_points[:, 2],
               s=50, cmap=cmap, marker=markers[i], label=f'Cluster {i + 1}')

# Plot cluster centers with a color that is distinguishable
ax.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], km.cluster_centers_[:, 2],
           c='blue', marker='o', s=300, edgecolors='black', linewidth=2, label='Cluster Centers')

# Adjust axis labels
ax.set_xlabel(df.columns[5])
ax.set_ylabel(df.columns[9])
ax.set_zlabel(df.columns[3])

# Set view angles
ax.azim = -80
ax.dist = 9
ax.elev = 10

# Add legend
legend = ax.legend(loc="center left", title="Clusters")
ax.add_artist(legend)

fig.tight_layout(pad=-2.0)
fig.savefig('cluster_3Dplot_custom_markers.png')
plt.show()

#  original DataFrame
df_clusters = df.copy()

# Add a new column to store cluster labels
df_clusters['Cluster_Labels'] = labels  

# Print the content of each cluster separately
for cluster_num in range(km.n_clusters):
    cluster_data = df_clusters[df_clusters['Cluster_Labels'] == cluster_num]
    
    print(f'\nCluster {cluster_num} of KMeans model\n{cluster_data["country"].values}\n')
